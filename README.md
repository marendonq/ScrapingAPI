# WEBSCRAPING\_API

Documentaci√≥n t√©cnica y gu√≠a de uso del servicio de scraping para **Casa Ferretera**. Este proyecto expone una API con FastAPI que recorre p√°ginas de listado de productos, extrae datos estructurados (JSON‚ÄëLD), enriquece cada producto con su detalle, y persiste todo en **SQLite**.

---

## ‚ú® Caracter√≠sticas clave

* **API REST** con FastAPI (/health, /scrape, /products)
* **Scraping** basado en **JSON‚ÄëLD** (`ItemList`, `Product`) y *fallback* al DOM para breadcrumbs
* **Concurrencia** as√≠ncrona controlada
* **Persistencia** en SQLite con *upsert* idempotente por `url_producto`
* **Modelo de dominio** tipado con Pydantic (Product, Category)
* **Configuraci√≥n** centralizada v√≠a `pydantic-settings` (prefijo `SCRAPER_`)
* **Categorizaci√≥n** jer√°rquica y relaci√≥n `product_categories`

---

## üß± Arquitectura y m√≥dulos

```
WEBSCRAPING_API/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # Endpoints FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ domain/              # Modelos y contratos de repositorios
‚îÇ   ‚îú‚îÄ‚îÄ http/                # Cliente HTTP (httpx)
‚îÇ   ‚îú‚îÄ‚îÄ parsers/             # Parseo de JSON-LD y breadcrumbs
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Caso de uso de scraping
‚îÇ   ‚îú‚îÄ‚îÄ storage/             # Implementaciones de repositorios (SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Utilidades de texto/HTML
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # App factory y wiring
‚îÇ   ‚îî‚îÄ‚îÄ .gitignore
‚îÇ
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ app.db               # Base de datos SQLite (ruta por defecto)
```

### Flujo de alto nivel

1. **/scrape** ‚Üí `ScrapingService.scrape_all()`
2. Descarga de p√°ginas de listado y extracci√≥n de productos desde JSON‚ÄëLD
3. Enriquecimiento por detalle del producto: breadcrumbs ‚Üí categor√≠as; opcionalmente campos del Product JSON‚ÄëLD
4. *Upsert* de productos y vinculaci√≥n con categor√≠as en SQLite

---

## ‚öôÔ∏è Configuraci√≥n

Definida en `app/config.py` mediante `Settings`. Se puede sobreescribir por variables de entorno con prefijo **`SCRAPER_`** (ej. `SCRAPER_DB_PATH`).

* `APP_NAME`: nombre de la app (para FastAPI docs)
* `BASE_URL`: dominio inicial (p. ej. `https://www.casaferretera.com`)
* `START_PATH`: ruta de listado con `page=1`
* **DB**: `DB_PATH` ‚Üí `data/app.db` por defecto
* **HTTP**: `USER_AGENT`, `HTTP2`, *timeouts*, *limits*, `FOLLOW_REDIRECTS`
* **Scraping**: `CONCURRENCY`, `PAGE_DELAY_SECS`, `MAX_PAGES`, `ENRICH_FROM_PRODUCT_DETAIL`

> Ejemplo: `SCRAPER_MAX_PAGES=3 uvicorn app.main:app --reload`

---

## üîå HTTP Client

`app/http/client.py` implementa un `httpx.AsyncClient` con *lifespan* propio, `User-Agent` configurable y l√≠mites de conexiones. `get_text(url)` devuelve el HTML y propaga errores HTTP.

---

## üß† Parsing (JSON‚ÄëLD y breadcrumbs)

`app/parsers/jsonld.py`:

* Extrae bloques `application/ld+json`
* **Productos** desde `ItemList.itemListElement[].item` (o `Product` √∫nico)
* Normaliza `name`, `description` (HTML ‚Üí texto), `sku`, `brand`, `offers.price` y `priceCurrency`
* **Breadcrumbs**:

  * Intenta `BreadcrumbList` en JSON‚ÄëLD
  * *Fallback* a selectores del DOM (`div[data-testid="breadcrumb"] a...`)
* Genera *slugs* desde URL o `slugify(name)`

---

## üóÉÔ∏è Persistencia y esquema

**SQLite** con *migraci√≥n* autom√°tica al iniciar la app.

Tablas principales:

* `categories(id, name, slug UNIQUE, url)` + √≠ndice por `slug`
* `products(id, nombre, descripcion, precio, divisa, url_producto UNIQUE, image_url, sku, brand, categoria, codigo_categoria)` + √≠ndices por `sku` y `codigo_categoria`
* `product_categories(product_id, category_id, level)` (PK compuesta)

**Upsert**: conflicto por `url_producto` actualiza campos no nulos del nuevo registro.

Ruta de la DB: si `DB_PATH` es relativa (p. ej. `data/app.db`), se resuelve al **root del proyecto** y se crean directorios si faltan.

---

## üß© Repositorios y dominio

* **Modelos** (`app/domain/models.py`): `Product`, `Category` (Pydantic), con URL tipadas y listas de categor√≠as por producto.
* **Contratos** (`app/domain/repositories.py`): `ProductRepository`, `CategoryRepository` (Protocol) + implementaciones **InMemory** para pruebas.
* **SQLite** (`app/storage/sqlite.py`): `SQLiteDb` (con *lifespan*, PRAGMAs, migraci√≥n) y repos `SQLiteProductRepository` / `SQLiteCategoryRepository`.

  * `ensure_path(crumbs)` crea/actualiza categor√≠as por `slug` y retorna objetos con `id`
  * `save_many(products)` hace *upsert* y re‚Äëasigna enlaces `product_categories` con niveles `level` en orden de breadcrumb

---

## üß™ Servicio de scraping

`app/services/scraping_service.py`:

* `_page_url(page)` sustituye `page=...` en `START_PATH`
* `_fetch_list_page(page)` ‚Üí `parse_itemlist_products` ‚Üí `Product` b√°sicos
* `_enrich_from_detail(products)` descarga detalle concurrente (sem√°foro = `CONCURRENCY`), obtiene breadcrumbs ‚Üí categor√≠as (y opcionalmente `descripcion`, `image_url`, `sku`, `brand` desde `Product` del detalle)
* `scrape_all()` pagina hasta agotar o `MAX_PAGES`, aplica `PAGE_DELAY_SECS`, enriquece, asigna `id` temporales y persiste todo
* `list_products()` retorna los productos desde la DB

---

## üåê API HTTP

**Montaje**: `app/main.py` crea la app, configura *lifespans* de HTTP y DB, y registra rutas.

Rutas (`app/api/routes.py`):

* `GET /health` ‚Üí `{ "ok": true }`
* `POST /scrape` ‚Üí `List[Product]` (ejecuta scraping completo)
* `GET /products` ‚Üí `List[Product]` (lee desde SQLite)

**Ejemplos**

```bash
# Iniciar
uvicorn app.main:app --reload

# Salud
curl http://127.0.0.1:8000/health

# Ejecutar scraping (puede tardar seg√∫n MAX_PAGES / red)
curl -X POST http://127.0.0.1:8000/scrape | jq '.'

# Consultar productos
curl http://127.0.0.1:8000/products | jq '.'
```

---

## üß∞ Utilidades

`app/utils/text.py`:

* `normalize_whitespace` (limpieza), `html_to_text` (desescape + strip tags), `to_float_price` (normaliza precio), `slugify` (para slugs estables)

---

## üöÄ Puesta en marcha

1. **Python 3.11+** recomendado
2. Crear entorno y deps (ejemplo):

   ```bash
   python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
   pip install fastapi uvicorn[standard] httpx selectolax pydantic pydantic-settings aiosqlite
   ```
3. Variables opcionales (ejemplos):

   ```bash
   export SCRAPER_MAX_PAGES=2
   export SCRAPER_CONCURRENCY=8
   export SCRAPER_DB_PATH=data/app.db
   ```
4. Iniciar API: `uvicorn app.main:app --reload`

La primera ejecuci√≥n crear√° la base de datos y tablas si no existen.

---

## üßØ Errores comunes y resoluci√≥n

* **"HttpClient not initialized" / "SQLiteDb not initialized"**: invocar siempre dentro del ciclo de vida de la app (la `main.py` ya lo hace). Evita usar los clientes fuera del *lifespan*.
* **Bloqueos de sitio**: ajustar `USER_AGENT`, `CONCURRENCY`, `PAGE_DELAY_SECS`. Considerar *retry/backoff* si el sitio limita tr√°fico.
* **Cambios de marca/DOM**: actualizar selectores de fallback o parseo de JSON‚ÄëLD.
